{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bf6f9e8",
   "metadata": {},
   "source": [
    "# Getting Started with Avalan CLI\n",
    "\n",
    "This tutorial walks through the Avalan command line interface (CLI) to build agents and models. You'll prepare your environment, explore the main command groups, and learn how to search for models, adjust tokenizers, manage memory, and serve agents. Every step uses CLI commands so you can follow along in your own terminal.\n",
    "\n",
    "**What you'll learn**\n",
    "\n",
    "- Installing dependencies and confirming the CLI works\n",
    "- Inspecting and installing models from supported hubs\n",
    "- Customizing tokenizers and caching downloaded weights\n",
    "- Running agents with tools, memories, and configuration files\n",
    "- Serving or deploying agents once you're ready to integrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99e4cc5",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running the commands below, make sure you have:\n",
    "\n",
    "1. **Python 3.11+ and [Poetry](https://python-poetry.org/)** installed on your machine.\n",
    "2. Access to any private model hubs you intend to use (set environment variables such as `HF_TOKEN` when necessary).\n",
    "3. GPU drivers and CUDA/cuDNN installed if you plan to run larger transformer models locally.\n",
    "\n",
    "> ℹ️ **Tip:** Avalan reads most CLI options from environment variables as well. Run `poetry run avalan --help` to see the environment variable that corresponds to each flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a681ec",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "Run the following command in the project root to create the virtual environment and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c814e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry install --sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1832b616",
   "metadata": {},
   "source": [
    "### Verify the CLI is available\n",
    "\n",
    "The first invocation can take a little while because Avalan lazily loads optional dependencies such as Hugging Face Transformers and Diffusers. Subsequent runs are much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run avalan --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bba87a",
   "metadata": {},
   "source": [
    "## Finding a Model for Your Task\n",
    "\n",
    "Use `avalan model search` to locate models that match your needs. You can filter by task, name, modality, and other metadata. Start by checking the help text to discover all available options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c91d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run avalan model search --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4805dd",
   "metadata": {},
   "source": [
    "Search for a sentiment analysis model and limit the results to five entries:\n",
    "\n",
    "```\n",
    "poetry run avalan model search --search sentiment --limit 5\n",
    "```\n",
    "\n",
    "Once you identify a model, you can display its details and verify you have access before downloading weights:\n",
    "\n",
    "```\n",
    "poetry run avalan model display <model-id>\n",
    "```\n",
    "\n",
    "When you're ready to try a model locally, download it to the cache or a custom directory:\n",
    "\n",
    "```\n",
    "poetry run avalan model install <model-id> --revision main --local-dir ./models\n",
    "```\n",
    "\n",
    "For a quick quality check, stream tokens directly from the model without building an agent:\n",
    "\n",
    "```\n",
    "poetry run avalan model run <model-id> --prompt \"Hello there\" --max-new-tokens 64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f95ec66",
   "metadata": {},
   "source": [
    "## Learning About Tokenizers\n",
    "\n",
    "Tokenizers break text into tokens understood by models. The `tokenizer` command lets you inspect, extend, and persist tokenization rules. Check the help output to review all flags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61374240",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run avalan tokenizer --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c4e295",
   "metadata": {},
   "source": [
    "To see how a tokenizer splits text, pipe input through the command:\n",
    "\n",
    "```\n",
    "echo 'Hello world' | poetry run avalan tokenizer -t gpt2 --skip-hub-access-check --no-repl\n",
    "```\n",
    "\n",
    "Use `--special-token` or `--token` to extend vocabularies, `--save` to write the modified tokenizer to disk, and `--tokenizer-subfolder` when working with repositories that ship multiple tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206219b3",
   "metadata": {},
   "source": [
    "## Managing the Model Cache\n",
    "\n",
    "Caching avoids repeated downloads and lets you prune unused revisions. The `cache` subcommands expose the most common operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7097422",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run avalan cache --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f190bb4",
   "metadata": {},
   "source": [
    "Typical workflows include:\n",
    "\n",
    "- **Download weights for offline work:**\n",
    "  ```\n",
    "poetry run avalan cache download meta-llama/Meta-Llama-3-8B-Instruct --workers 4\n",
    "```\n",
    "- **Inspect your cache:**\n",
    "  ```\n",
    "poetry run avalan cache list --summary\n",
    "```\n",
    "- **Remove specific revisions or everything for a model:**\n",
    "  ```\n",
    "poetry run avalan cache delete meta-llama/Meta-Llama-3-8B-Instruct --delete-revision main\n",
    "```\n",
    "\n",
    "All cache commands honor `--cache-dir`, so you can keep large weights on a secondary volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c2fea",
   "metadata": {},
   "source": [
    "## Using Tools with Models\n",
    "\n",
    "Avalan agents can call external tools while generating responses. When running an agent, add `--tool` arguments and enable event display to visualize tool calls. Review the available runtime options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b11ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run avalan agent run --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234fc36",
   "metadata": {},
   "source": [
    "For example, invoke a calculator tool and print event traces:\n",
    "\n",
    "```\n",
    "echo 'What is (4 + 6) * 5?' |     poetry run avalan agent run         --engine-uri meta-llama/Meta-Llama-3-8B-Instruct         --tool math.calculator         --display-events --quiet\n",
    "```\n",
    "\n",
    "The `--display-events` flag streams tool invocation events so you can follow how the agent reasons. To inspect recent conversations without re-running prompts, use `poetry run avalan agent message search --id <agent-id> --participant <uuid> --session <uuid>`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0652ae",
   "metadata": {},
   "source": [
    "## Keeping Memories Across Sessions\n",
    "\n",
    "Agents can maintain context with recent and persistent memories. Recent memory keeps a rolling window of messages, while persistent memory stores information in external backends.\n",
    "\n",
    "Use these flags when launching an agent:\n",
    "\n",
    "```\n",
    "poetry run avalan agent run --engine-uri <model-id>     --memory-recent     --memory-permanent-message 'postgresql://user:pass@localhost:5432/dbname'\n",
    "```\n",
    "\n",
    "Replace the DSN above with your own database connection string. With both options enabled, the agent recalls prior turns across runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3722565",
   "metadata": {},
   "source": [
    "### Indexing documents into memory\n",
    "\n",
    "Populate the memory store with documents before chatting so the agent can ground its answers. The `memory document index` command accepts local files or URLs and chunks them with either a text or code partitioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run avalan memory document index --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579beb9",
   "metadata": {},
   "source": [
    "Index a Markdown knowledge base stored on disk:\n",
    "\n",
    "```\n",
    "poetry run avalan memory document index docs/handbook.md     --model sentence-transformers/all-MiniLM-L6-v2     --namespace support --participant 00000000-0000-0000-0000-000000000000     --dsn postgresql://user:pass@localhost:5432/avalan --partition-max-tokens 256\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56742355",
   "metadata": {},
   "source": [
    "## Creating Agents via CLI Arguments and Configuration Files\n",
    "\n",
    "Agents can be built inline with CLI flags or from TOML configuration files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cea14c",
   "metadata": {},
   "source": [
    "### Scaffold a configuration file\n",
    "\n",
    "Interactively generate a blueprint using `agent init` and redirect it to disk:\n",
    "\n",
    "```\n",
    "poetry run avalan agent init > helper.toml\n",
    "```\n",
    "\n",
    "Re-run the command with `--name`, `--role`, or `--tool` flags to skip prompts in automated environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6878f4d9",
   "metadata": {},
   "source": [
    "### Inline agent\n",
    "\n",
    "```\n",
    "poetry run avalan agent run     --engine-uri meta-llama/Meta-Llama-3-8B-Instruct     --tool math.calculator     --memory-recent     --name 'Helper'     --role 'You are a helpful assistant named Helper.'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08340d1a",
   "metadata": {},
   "source": [
    "### Configuration file\n",
    "\n",
    "Create `my_agent.toml`:\n",
    "\n",
    "```\n",
    "[engine]\n",
    "uri = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "[run]\n",
    "name = \"Helper\"\n",
    "role = \"You are a helpful assistant named Helper.\"\n",
    "memory_recent = true\n",
    "[tool]\n",
    "enable = [\"math.calculator\"]\n",
    "```\n",
    "\n",
    "Launch it with:\n",
    "\n",
    "```\n",
    "poetry run avalan agent run my_agent.toml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e8e683",
   "metadata": {},
   "source": [
    "## Serving an Agent via the OpenAI API Endpoint\n",
    "\n",
    "Use `agent serve` to expose your agent on an OpenAI-compatible HTTP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d567b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run avalan agent serve --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d3e13",
   "metadata": {},
   "source": [
    "Start the server:\n",
    "\n",
    "```\n",
    "poetry run avalan agent serve my_agent.toml -vvv\n",
    "```\n",
    "\n",
    "The server listens on `http://localhost:9001/v1` by default.\n",
    "\n",
    "### Python Client\n",
    "```\n",
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://localhost:9001/v1\")\n",
    "chat = client.chat.completions.create(\n",
    "    model=\"openai\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello agent\"}],\n",
    ")\n",
    "print(chat.choices[0].message.content)\n",
    "```\n",
    "\n",
    "### TypeScript Client\n",
    "```\n",
    "import OpenAI from \"openai\";\n",
    "const client = new OpenAI({ baseURL: \"http://localhost:9001/v1\" });\n",
    "const chat = await client.chat.completions.create({\n",
    "  model: \"openai\",\n",
    "  messages: [{ role: \"user\", content: \"Hello agent\" }],\n",
    "});\n",
    "console.log(chat.choices[0].message?.content);\n",
    "```\n",
    "\n",
    "With the server running, both clients communicate with your agent using familiar OpenAI API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb565c8",
   "metadata": {},
   "source": [
    "## Deploying Agents to AWS (Optional)\n",
    "\n",
    "Automate provisioning with `deploy run`. Provide a deployment TOML that specifies your VPC, instance type, and the agents to publish. A minimal example looks like:\n",
    "\n",
    "```\n",
    "[aws]\n",
    "vpc = \"my-vpc\"\n",
    "instance = \"t3.large\"\n",
    "database = \"avalan-db\"\n",
    "pgsql = \"postgresql://user:pass@host:5432/avalan\"\n",
    "\n",
    "[agents]\n",
    "port = 9001\n",
    "publish = \"my_agent.toml\"\n",
    "```\n",
    "\n",
    "Run the deployment (requires AWS credentials in your environment):\n",
    "\n",
    "```\n",
    "poetry run avalan deploy run deployment.toml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb760b",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Explore additional tutorials in `docs/tutorials/` for in-depth walkthroughs.\n",
    "- Read `poetry run avalan train run --help` to experiment with fine-tuning workflows.\n",
    "- Combine agents, flows, and tools in automation pipelines to orchestrate complex tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
