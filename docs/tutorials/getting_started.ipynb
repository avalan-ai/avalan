{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\n# Getting Started with Avalan CLI\n\nThis tutorial walks through the Avalan command line interface (CLI) to build agents and models. We'll explore how to find suitable models, inspect tokenizers, use tools, retain memories, craft agents, and serve them through an OpenAI-compatible endpoint. Each step uses CLI commands so you can follow along in your own terminal.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\n## Finding a Model for Your Task\n\nUse `avalan model search` to locate models that match your needs. You can filter by task, name, or other metadata. The command below shows the help text, which lists all available options:\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "!poetry run avalan model search --help"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\nTo search for a sentiment analysis model and limit the results, run:\n\n```bash\npoetry run avalan model search --search sentiment --limit 5\n```\n\nOnce you identify a model, you can display its details:\n\n```bash\npoetry run avalan model display <model-id>\n```\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\n## Learning About Tokenizers\n\nTokenizers break text into tokens understood by models. The `tokenizer` command lets you inspect and modify tokenization.\n\nThe help output shows available flags:\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "!poetry run avalan tokenizer --help"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\nTo see how a tokenizer splits text, pipe input through the command:\n\n```bash\necho 'Hello world' | poetry run avalan tokenizer -t gpt2 --skip-hub-access-check --no-repl\n```\n\nUse `--special-token` or `--token` to extend vocabularies, and `--save` to write the modified tokenizer to disk.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\n## Using Tools with Models\n\nAvalan agents can call external tools while generating responses. When running an agent, add `--tool` arguments and enable event display to visualize tool calls.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "!poetry run avalan agent run --help"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\nFor example, the following command invokes a calculator tool and prints event traces:\n\n```bash\necho 'What is (4 + 6) * 5?' |     poetry run avalan agent run --engine-uri meta-llama/Meta-Llama-3-8B-Instruct         --tool math.calculator --display-events --quiet\n```\n\nThe `--display-events` flag streams tool invocation events so you can follow how the agent reasons.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\n## Keeping Memories Across Sessions\n\nAgents can maintain context with recent and persistent memories. Recent memory keeps a rolling window of messages, while persistent memory stores information in external backends.\n\nUse these flags when launching an agent:\n\n```bash\npoetry run avalan agent run --engine-uri <model-id>     --memory-recent     --memory-permanent-message 'postgresql://user:pass@localhost:5432/dbname'\n```\n\nReplace the DSN above with your own database connection string. With both options enabled, the agent recalls prior turns across runs.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\n## Creating Agents via CLI Arguments and Configuration Files\n\nAgents can be built inline with CLI flags or from TOML configuration files.\n\n### Inline Agent\n```bash\npoetry run avalan agent run     --engine-uri meta-llama/Meta-Llama-3-8B-Instruct     --tool math.calculator     --memory-recent     --name 'Helper'     --role 'You are a helpful assistant named Helper.'\n```\n\n### Configuration File\nCreate `my_agent.toml`:\n\n```toml\n[engine]\nuri = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n[run]\nname = \"Helper\"\nrole = \"You are a helpful assistant named Helper.\"\nmemory_recent = true\n[[tools]]\nid = \"math.calculator\"\n```\n\nLaunch it with:\n\n```bash\npoetry run avalan agent run my_agent.toml\n```\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\n## Serving an Agent via the OpenAI API Endpoint\n\nUse `agent serve` to expose your agent on an OpenAI-compatible HTTP server.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "!poetry run avalan agent serve --help"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\nStart the server:\n\n```bash\npoetry run avalan agent serve my_agent.toml -vvv\n```\n\nThe server listens on `http://localhost:9001/v1` by default.\n\n### Python Client\n```python\nfrom openai import OpenAI\nclient = OpenAI(base_url=\"http://localhost:9001/v1\")\nchat = client.chat.completions.create(\n    model=\"openai\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello agent\"}],\n)\nprint(chat.choices[0].message.content)\n```\n\n### TypeScript Client\n```typescript\nimport OpenAI from \"openai\";\nconst client = new OpenAI({ baseURL: \"http://localhost:9001/v1\" });\nconst chat = await client.chat.completions.create({\n  model: \"openai\",\n  messages: [{ role: \"user\", content: \"Hello agent\" }],\n});\nconsole.log(chat.choices[0].message?.content);\n```\n\nWith the server running, both clients communicate with your agent using familiar OpenAI API calls.\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
