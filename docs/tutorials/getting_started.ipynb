{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\n# Getting Started with Avalan CLI\n\nThis tutorial walks through the Avalan command line interface (CLI) to build agents and models. We'll explore how to find suitable models, inspect tokenizers, use tools, retain memories, craft agents, and serve them through an OpenAI-compatible endpoint. Each step uses CLI commands so you can follow along in your own terminal.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exploring the CLI\n",
    "\n",
    "Avalan exposes all functionality through the `avalan` entry point. Run `poetry run avalan --help` to inspect global options such as `--cache-dir`, `--device`, `--locale`, and `--help-full`, along with the available command groups (`agent`, `cache`, `deploy`, `flow`, `memory`, `model`, `tokenizer`, and `train`). Use `--help-full` when you want a single dump containing help for every subcommand.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!poetry run avalan --help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Finding and Inspecting Models\n",
    "\n",
    "Use `poetry run avalan model search --help` to review filters like `--search`, `--task`, `--filter`, `--language`, and `--limit`. Combine them to locate models tailored to your project. For example:\n",
    "\n",
    "```bash\n",
    "poetry run avalan model search --task text-generation --search llama --limit 5\n",
    "```\n",
    "\n",
    "After narrowing your choices, inspect metadata with `model display`:\n",
    "\n",
    "```bash\n",
    "poetry run avalan model display meta-llama/Meta-Llama-3-8B-Instruct --summary\n",
    "```\n",
    "\n",
    "Add `--sentence-transformer` when you need to view the model as a sentence encoder instead of a text generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "!poetry run avalan model search --help"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Installing and Running Models\n",
    "\n",
    "Download weights locally with `model install` so repeated runs do not hit the Hugging Face hub:\n",
    "\n",
    "```bash\n",
    "poetry run avalan model install meta-llama/Meta-Llama-3-8B-Instruct --workers 4\n",
    "```\n",
    "\n",
    "Stream responses directly from the command line. Pipe prompts through `model run` and add decoding flags like `--skip-special-tokens` or `--display-tokens` when you want richer output diagnostics:\n",
    "\n",
    "```bash\n",
    "echo \"Summarize the following article\" |       poetry run avalan model run meta-llama/Meta-Llama-3-8B-Instruct       --skip-special-tokens --display-tokens\n",
    "```\n",
    "\n",
    "When you no longer need a model, clean up disk space with `model uninstall` (use `--delete` for an actual removal instead of a dry run):\n",
    "\n",
    "```bash\n",
    "poetry run avalan model uninstall meta-llama/Meta-Llama-3-8B-Instruct --delete\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!poetry run avalan model run --help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Learning About Tokenizers\n",
    "\n",
    "Tokenizers break text into tokens understood by models. The `tokenizer` command requires a tokenizer identifier via `-t/--tokenizer` and supports mutation flags such as `--special-token`, `--token`, and `--save` for persisting changes.\n",
    "\n",
    "The help output shows every option:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "!poetry run avalan tokenizer --help"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To see how a tokenizer splits text, pipe input through the command:\n",
    "\n",
    "```bash\n",
    "echo 'Hello world' | poetry run avalan tokenizer -t gpt2 --skip-hub-access-check --no-repl\n",
    "```\n",
    "\n",
    "Add `--special-token` or `--token` repeatedly to extend vocabularies, then provide `--save /path/to/tokenizer` to write the adjusted tokenizer to disk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Managing the Model Cache\n",
    "\n",
    "`avalan cache` lets you prime or inspect local weights. Download artifacts without running a model by calling `cache download` with the same arguments accepted by `model install`:\n",
    "\n",
    "```bash\n",
    "poetry run avalan cache download meta-llama/Meta-Llama-3-8B-Instruct --workers 4\n",
    "```\n",
    "\n",
    "Review cached revisions with `cache list --summary`, or remove entries using `cache delete -m <model-id>` and add `--delete` when you are sure you want to reclaim space.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!poetry run avalan cache list --help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Running Agents with Tools\n",
    "\n",
    "Avalan agents can call external tools while generating responses. Review available switches with `agent run --help`. When enabling tools, combine `--tool` (for specific tool IDs) or `--tools` (to enable by namespace) with `--tools-confirm` if you want to approve each call interactively.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "!poetry run avalan agent run --help"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For example, the following command invokes a calculator tool, prints event traces, and displays the live tool panel:\n",
    "\n",
    "```bash\n",
    "echo 'What is (4 + 6) * 5?' |       poetry run avalan agent run --engine-uri meta-llama/Meta-Llama-3-8B-Instruct       --tool math.calculator --display-events --display-tools --quiet\n",
    "```\n",
    "\n",
    "The `--display-events` and `--display-tools` flags stream tool invocation events so you can follow how the agent reasons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Inspecting Agent Memories\n",
    "\n",
    "Use `avalan agent message search` to retrieve items from an agent's permanent message memory. Provide the agent specification file (or inline settings such as `--engine-uri`), then identify the agent/session pair:\n",
    "\n",
    "```bash\n",
    "poetry run avalan agent message search my_agent.toml       --id <agent-id> --participant <participant-id> --session <session-id>       --function l2_distance --limit 5\n",
    "```\n",
    "\n",
    "Swap `--function` with any value from `cosine_distance`, `inner_product`, `l1_distance`, or `l2_distance` to mirror the similarity metric stored in your database.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!poetry run avalan agent message search --help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Keeping Memories Across Sessions\n",
    "\n",
    "Agents can maintain context with recent and persistent memories. Recent memory keeps a rolling window of messages, while persistent memory stores information in external backends.\n",
    "\n",
    "Use these flags when launching an agent:\n",
    "\n",
    "```bash\n",
    "poetry run avalan agent run --engine-uri <model-id>       --memory-recent --memory-permanent-message 'postgresql://user:pass@localhost:5432/dbname'       --load-recent-messages-limit 20\n",
    "```\n",
    "\n",
    "Provide multiple permanent stores with `--memory-permanent namespace@dsn` and fine-tune chunking via `--memory-engine-model-id`, `--memory-engine-window`, `--memory-engine-overlap`, and `--memory-engine-max-tokens` so embeddings match your storage strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Creating Agents via CLI Arguments and Configuration Files\n",
    "\n",
    "Agents can be built inline with CLI flags or from TOML configuration files.\n",
    "\n",
    "### Inline Agent\n",
    "```bash\n",
    "poetry run avalan agent run       --engine-uri meta-llama/Meta-Llama-3-8B-Instruct       --tool math.calculator --memory-recent       --name 'Helper' --role 'You are a helpful assistant named Helper.'\n",
    "```\n",
    "\n",
    "### Configuration File\n",
    "Create `my_agent.toml`:\n",
    "\n",
    "```toml\n",
    "[engine]\n",
    "uri = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "[run]\n",
    "name = \"Helper\"\n",
    "role = \"You are a helpful assistant named Helper.\"\n",
    "memory_recent = true\n",
    "\n",
    "[tool]\n",
    "enable = [\"math.calculator\"]\n",
    "```\n",
    "\n",
    "Launch it with:\n",
    "\n",
    "```bash\n",
    "poetry run avalan agent run my_agent.toml\n",
    "```\n",
    "\n",
    "Generate a starter TOML interactively with `poetry run avalan agent init --name Helper --role \"Helpful assistant\" --tool math.calculator`, then tweak the saved blueprint before running it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Serving an Agent via the OpenAI API Endpoint\n",
    "\n",
    "Use `agent serve` to expose your agent on an OpenAI-compatible HTTP server. Customize the listener with `--host`, `--port`, or change the API prefixes via `--openai-prefix`, `--mcp-prefix`, and `--a2a-prefix`. Add `--cors-origin` or `--cors-origin-regex` when integrating with browsers, and `--reload` to auto-restart during development.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "!poetry run avalan agent serve --help"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Start the server:\n",
    "\n",
    "```bash\n",
    "poetry run avalan agent serve my_agent.toml -vvv\n",
    "```\n",
    "\n",
    "The server listens on `http://localhost:9001/v1` by default.\n",
    "\n",
    "### Python Client\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:9001/v1\")\n",
    "chat = client.chat.completions.create(\n",
    "    model=\"openai\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello agent\"}],\n",
    ")\n",
    "print(chat.choices[0].message.content)\n",
    "```\n",
    "\n",
    "### TypeScript Client\n",
    "```typescript\n",
    "import OpenAI from \"openai\";\n",
    "\n",
    "const client = new OpenAI({ baseURL: \"http://localhost:9001/v1\" });\n",
    "const chat = await client.chat.completions.create({\n",
    "  model: \"openai\",\n",
    "  messages: [{ role: \"user\", content: \"Hello agent\" }],\n",
    "});\n",
    "console.log(chat.choices[0].message?.content);\n",
    "```\n",
    "\n",
    "With the server running, both clients communicate with your agent using familiar OpenAI API calls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Running Flows and Trainings\n",
    "\n",
    "Avalan can orchestrate multi-step automations and fine-tuning jobs directly from the CLI. Point `flow run` at a flow definition (TOML or YAML) to execute your sequence of tasks:\n",
    "\n",
    "```bash\n",
    "poetry run avalan flow run flows/my_flow.toml\n",
    "```\n",
    "\n",
    "To launch a training recipe, supply the training file to `train run`:\n",
    "\n",
    "```bash\n",
    "poetry run avalan train run trainings/my_training.toml\n",
    "```\n",
    "\n",
    "Both commands understand the same global flags as `avalan` (for example `--cache-dir` or `--locale`), so you can reuse environment overrides consistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!poetry run avalan flow run --help\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!poetry run avalan train run --help\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}